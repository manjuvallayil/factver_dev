{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset,load_from_disk\n",
    "from tensorboard import program\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Load formatted dataset from disk\n",
    "dataset_path = '/home/qsh5523/Documents/factver/factver_dataset'\n",
    "dataset = load_from_disk(dataset_path)\n",
    "print('Factver Dataset: ', dataset['train'][:1])\n",
    "\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\" \n",
    "\n",
    "# The instruction dataset to use\n",
    "#dataset_name = 'factver'\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"Llama-2-7b-chat-finetune\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n",
    "# Load dataset (you can process it here)\n",
    "#dataset = load_dataset(dataset_name, split=\"train\")\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# Test the base model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output:  <s>[INST] New Zealand wants to create a carbon trading system? [/INST]\n",
      " nobody knows what it is,\n",
      "nobody knows what it is,\n",
      "nobody knows what it is,\n",
      "nobody knows what it is,\n",
      "nobody knows what it is,\n",
      "nobody knows what it is,\n",
      "nobody knows what it is,\n",
      "nobody knows what it is,\n",
      "nobody knows what it is,\n",
      "nobody knows what it is,\n",
      "nobody knows what it is,\n",
      "nobody knows what it is,\n",
      "nobody knows what it is,\n",
      "nobody knows what it is,\n",
      "nobody knows what it is,\n",
      "nobody knows what it is,\n",
      "nobody knows what it is,\n",
      "nobody knows what it is,\n",
      "nobody knows what it is,\n",
      "nobody knows what it is,\n",
      "nob\n"
     ]
    }
   ],
   "source": [
    "# Test text generation pipeline with base model\n",
    "prompt = \"New Zealand wants to create a carbon trading system?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print('Model Output: ', result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qsh5523/miniconda3/envs/factver_env/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "/home/qsh5523/miniconda3/envs/factver_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 3102/3102 [00:00<00:00, 15270.20 examples/s]\n",
      "/home/qsh5523/miniconda3/envs/factver_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.443, 'learning_rate': 0.00019999912736337437, 'epoch': 0.03}\n",
      "{'loss': 0.8458, 'learning_rate': 0.00019941067653142657, 'epoch': 0.06}\n",
      "{'loss': 0.889, 'learning_rate': 0.00019773884198663702, 'epoch': 0.1}\n",
      "{'loss': 0.6346, 'learning_rate': 0.00019500184348547042, 'epoch': 0.13}\n",
      "{'loss': 0.8801, 'learning_rate': 0.0001912295090071911, 'epoch': 0.16}\n",
      "{'loss': 0.6395, 'learning_rate': 0.00018646294968669685, 'epoch': 0.19}\n",
      "{'loss': 0.7972, 'learning_rate': 0.0001807541117828232, 'epoch': 0.23}\n",
      "{'loss': 0.5902, 'learning_rate': 0.00017416521056479577, 'epoch': 0.26}\n",
      "{'loss': 0.8385, 'learning_rate': 0.00016676805228637128, 'epoch': 0.29}\n",
      "{'loss': 0.5995, 'learning_rate': 0.00015864325163683431, 'epoch': 0.32}\n",
      "{'loss': 0.7677, 'learning_rate': 0.00014987935319711842, 'epoch': 0.35}\n",
      "{'loss': 0.5772, 'learning_rate': 0.0001405718664754764, 'epoch': 0.39}\n",
      "{'loss': 0.7732, 'learning_rate': 0.00013082222503894085, 'epoch': 0.42}\n",
      "{'loss': 0.5205, 'learning_rate': 0.00012073668108402565, 'epoch': 0.45}\n",
      "{'loss': 0.7227, 'learning_rate': 0.00011042514749370418, 'epoch': 0.48}\n",
      "{'loss': 0.5294, 'learning_rate': 0.0001, 'epoch': 0.52}\n",
      "{'loss': 0.7757, 'learning_rate': 8.957485250629584e-05, 'epoch': 0.55}\n",
      "{'loss': 0.5631, 'learning_rate': 7.926331891597436e-05, 'epoch': 0.58}\n",
      "{'loss': 0.7252, 'learning_rate': 6.917777496105917e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5115, 'learning_rate': 5.9428133524523646e-05, 'epoch': 0.64}\n",
      "{'loss': 0.7502, 'learning_rate': 5.01206468028816e-05, 'epoch': 0.68}\n",
      "{'loss': 0.5083, 'learning_rate': 4.135674836316569e-05, 'epoch': 0.71}\n",
      "{'loss': 0.7225, 'learning_rate': 3.323194771362875e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5116, 'learning_rate': 2.5834789435204243e-05, 'epoch': 0.77}\n",
      "{'loss': 0.7095, 'learning_rate': 1.9245888217176854e-05, 'epoch': 0.81}\n",
      "{'loss': 0.4904, 'learning_rate': 1.3537050313303179e-05, 'epoch': 0.84}\n",
      "{'loss': 0.7364, 'learning_rate': 8.770490992808911e-06, 'epoch': 0.87}\n",
      "{'loss': 0.4785, 'learning_rate': 4.998156514529595e-06, 'epoch': 0.9}\n",
      "{'loss': 0.7165, 'learning_rate': 2.261158013362996e-06, 'epoch': 0.93}\n",
      "{'loss': 0.4781, 'learning_rate': 5.893234685734439e-07, 'epoch': 0.97}\n",
      "{'loss': 0.5951, 'learning_rate': 8.726366256261998e-10, 'epoch': 1.0}\n",
      "{'train_runtime': 550.1094, 'train_samples_per_second': 5.639, 'train_steps_per_second': 1.411, 'train_loss': 0.7194100774286949, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load the TensorBoard Notebook Extension\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir results/runs\n",
    "\n",
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"New Zealand wants to create a carbon trading system?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print('Model Output: ', result[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty VRAM\n",
    "del model\n",
    "del pipe\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UFT-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!huggingface-cli login # token = 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\n",
    "\n",
    "model.push_to_hub(\"manjuvallayil/Llama-2-7b-chat-finetune\", check_pr=True)\n",
    "tokenizer.push_to_hub(\"manjuvallayil/Llama-2-7b-chat-finetune\",check_pr=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factver_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
